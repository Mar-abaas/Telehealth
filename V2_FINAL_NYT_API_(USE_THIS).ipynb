{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "BDE2pj80Lhn7"
      },
      "outputs": [],
      "source": [
        "## Imports\n",
        "import urllib\n",
        "import json\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import urllib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "FtBXz0WLMecr"
      },
      "outputs": [],
      "source": [
        "## Set-up values\n",
        "baseurl=\"https://api.nytimes.com/svc/search/v2/articlesearch.json?\"\n",
        "apikey=\"lPodSvfatuofOIu45ljamN3cq4ADGFOM\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "j9bBt9arM4jo"
      },
      "outputs": [],
      "source": [
        "## Creating Lists (Final Lists)\n",
        "# date_params = [\"2023-01-02\", \"2023-03-31\", \"2023-04-01\", \"2023-06-30\", \"2023-07-01\", \"2023-09-30\", \"2023-10-01\", \"2023-12-31\"]\n",
        "\n",
        "## Split into 5 so it can be run in separate chunks\n",
        "states1 = [\"Alabama\", \"Alaska\", \"Arizona\", \"Arkansas\", \"California\", \"Colorado\", \"Connecticut\", \"Delaware\", \"District of Columbia\", \"Florida\"]\n",
        "states2 = [\"Georgia\", \"Hawaii\", \"Idaho\", \"Illinois\", \"Indiana\", \"Iowa\", \"Kansas\", \"Kentucky\", \"Louisiana\", \"Maine\"]\n",
        "states3 = [\"Maryland\", \"Massachusetts\", \"Michigan\", \"Minnesota\", \"Mississippi\", \"Missouri\", \"Montana\", \"Nebraska\", \"Nevada\", \"New Hampshire\"]\n",
        "states4 = [\"New Jersey\", \"New Mexico\", \"New York\", \"North Carolina\", \"North Dakota\", \"Ohio\", \"Oklahoma\", \"Oregon\", \"Pennsylvania\", \"Rhode Island\"]\n",
        "states5 = [\"South Carolina\", \"South Dakota\", \"Tennessee\", \"Texas\", \"Utah\", \"Vermont\", \"Virginia\", \"Washington\", \"West Virginia\", \"Wisconsin\", \"Wyoming\"]\n",
        "\n",
        "quarters = [\"Q1\", \"\", \"Q2\", \"\", \"Q3\", \"\", \"Q4\", \"\"]\n",
        "quarter_dates = [\"2023-01-01\", \"2023-03-21\", \"2023-04-01\", \"2023-06-30\", \"2023-07-01\", \"2023-09-30\", \"2023-10-01\", \"2023-12-31\"]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run everything above this cell before running any cells below"
      ],
      "metadata": {
        "id": "4bSK09N2jGvn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6njufIsuSY0e",
        "outputId": "e247842e-99a3-4c14-e29b-509a43a8c8cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Alabama Q1 1\n",
            "Alabama Q1 2\n",
            "Alabama Q1 3\n",
            "Alabama Q1 4\n",
            "Alabama Q1 5\n",
            "Alabama Q1 6\n",
            "Alabama Q1 7\n",
            "Alabama Q1 8\n",
            "Alabama Q1 9\n",
            "Alabama Q1 10\n",
            "Alabama Q1 11\n",
            "Alabama Q1 12\n",
            "Alabama Q1 13\n",
            "Alabama Q1 14\n",
            "Alabama Q1 15\n",
            "Alabama Q1 16\n",
            "Alabama Q1 17\n",
            "Alabama Q1 18\n",
            "Alabama Q1 19\n",
            "Alabama Q1 20\n",
            "Alabama Q2 1\n",
            "Alabama Q2 2\n",
            "Alabama Q2 3\n",
            "Alabama Q2 4\n",
            "Alabama Q2 5\n",
            "Alabama Q2 6\n",
            "Alabama Q2 7\n",
            "Alabama Q2 8\n",
            "Alabama Q2 9\n",
            "Alabama Q2 10\n",
            "Alabama Q2 11\n",
            "Alabama Q2 12\n",
            "Alabama Q2 13\n",
            "Alabama Q2 14\n",
            "Alabama Q2 15\n",
            "Alabama Q2 16\n",
            "Alabama Q2 17\n",
            "Alabama Q2 18\n",
            "Alabama Q2 19\n",
            "Alabama Q2 20\n",
            "Alabama Q3 1\n",
            "Alabama Q3 2\n",
            "Alabama Q3 3\n",
            "Alabama Q3 4\n",
            "Alabama Q3 5\n",
            "Alabama Q3 6\n",
            "Alabama Q3 7\n",
            "Alabama Q3 8\n",
            "Alabama Q3 9\n",
            "Alabama Q3 10\n",
            "Alabama Q3 11\n",
            "Alabama Q3 12\n",
            "Alabama Q3 13\n",
            "Alabama Q3 14\n",
            "Alabama Q3 15\n",
            "Alabama Q3 16\n",
            "Alabama Q3 17\n",
            "Alabama Q3 18\n",
            "Alabama Q3 19\n"
          ]
        }
      ],
      "source": [
        "## API Search Code 1\n",
        "df = pd.DataFrame({\"State\": [], \"Quarter\": [], \"Text\": []})\n",
        "\n",
        "for state in states1:\n",
        "  for quarter_iter in range(0, len(quarter_dates), 2):\n",
        "    for page in range(1, 21):\n",
        "      pa={\"api-key\":apikey, \"q\": \"Health, \" + state, \"begin_date\": quarter_dates[quarter_iter], \"end_date\": quarter_dates[quarter_iter + 1], \"page\": page}\n",
        "      url2check = baseurl + urllib.parse.urlencode(pa)\n",
        "      result = urllib.request.urlopen(url2check).read()\n",
        "      resd = json.loads(result)\n",
        "      if len(resd['response']['docs']) < 10:\n",
        "        hit_num = len(resd['response']['docs'])\n",
        "      else:\n",
        "        hit_num = 10\n",
        "\n",
        "      if hit_num == 0:\n",
        "        new_empty_row = [state, quarters[quarter_iter], \"\"]\n",
        "        #print(new_empty_row)\n",
        "        df.loc[len(df.index)] = new_empty_row\n",
        "      else:\n",
        "        for i in range(0, hit_num):\n",
        "          abstract = resd['response']['docs'][i][\"abstract\"]\n",
        "          lead_paragraph = resd['response']['docs'][i][\"lead_paragraph\"]\n",
        "          pub_date = resd['response']['docs'][i][\"pub_date\"]\n",
        "          combined_text = abstract + \" \" + lead_paragraph\n",
        "          new_row = [state, quarters[quarter_iter], combined_text]\n",
        "          #print(new_row)\n",
        "          df.loc[len(df.index)] = new_row\n",
        "      print(state, quarters[quarter_iter], page)\n",
        "      time.sleep(12)\n",
        "\n",
        "df.to_csv('list1.csv', index=False)\n",
        "print(\"Done\")\n",
        "\n",
        "## DOWNLOAD THE CSVs AND UPLOAD TO GITHUB SO YOU DON'T HAVE TO RUN IT AGAIN"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## API Search Code 2\n",
        "df = pd.DataFrame({\"State\": [], \"Quarter\": [], \"Text\": []})\n",
        "\n",
        "for state in states2:\n",
        "  for quarter_iter in range(0, len(quarter_dates), 2):\n",
        "    for page in range(1, 21):\n",
        "      pa={\"api-key\":apikey, \"q\": \"Health, \" + state, \"begin_date\": quarter_dates[quarter_iter], \"end_date\": quarter_dates[quarter_iter + 1], \"page\": page}\n",
        "      url2check = baseurl + urllib.parse.urlencode(pa)\n",
        "      result = urllib.request.urlopen(url2check).read()\n",
        "      resd = json.loads(result)\n",
        "      if len(resd['response']['docs']) < 10:\n",
        "        hit_num = len(resd['response']['docs'])\n",
        "      else:\n",
        "        hit_num = 10\n",
        "\n",
        "      if hit_num == 0:\n",
        "        new_empty_row = [state, quarters[quarter_iter], \"\"]\n",
        "        #print(new_empty_row)\n",
        "        df.loc[len(df.index)] = new_empty_row\n",
        "      else:\n",
        "        for i in range(0, hit_num):\n",
        "          abstract = resd['response']['docs'][i][\"abstract\"]\n",
        "          lead_paragraph = resd['response']['docs'][i][\"lead_paragraph\"]\n",
        "          pub_date = resd['response']['docs'][i][\"pub_date\"]\n",
        "          combined_text = abstract + \" \" + lead_paragraph\n",
        "          new_row = [state, quarters[quarter_iter], combined_text]\n",
        "          #print(new_row)\n",
        "          df.loc[len(df.index)] = new_row\n",
        "      print(state, quarters[quarter_iter], page)\n",
        "      time.sleep(12)\n",
        "\n",
        "df.to_csv('list2.csv', index=False)\n",
        "print(\"Done\")\n",
        "\n",
        "## DOWNLOAD THE CSVs AND UPLOAD TO GITHUB SO YOU DON'T HAVE TO RUN IT AGAIN"
      ],
      "metadata": {
        "id": "BsW4nvbsh6RW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## API Search Code 3\n",
        "df = pd.DataFrame({\"State\": [], \"Quarter\": [], \"Text\": []})\n",
        "\n",
        "for state in states3:\n",
        "  for quarter_iter in range(0, len(quarter_dates), 2):\n",
        "    for page in range(1, 21):\n",
        "      pa={\"api-key\":apikey, \"q\": \"Health, \" + state, \"begin_date\": quarter_dates[quarter_iter], \"end_date\": quarter_dates[quarter_iter + 1], \"page\": page}\n",
        "      url2check = baseurl + urllib.parse.urlencode(pa)\n",
        "      result = urllib.request.urlopen(url2check).read()\n",
        "      resd = json.loads(result)\n",
        "      if len(resd['response']['docs']) < 10:\n",
        "        hit_num = len(resd['response']['docs'])\n",
        "      else:\n",
        "        hit_num = 10\n",
        "\n",
        "      if hit_num == 0:\n",
        "        new_empty_row = [state, quarters[quarter_iter], \"\"]\n",
        "        #print(new_empty_row)\n",
        "        df.loc[len(df.index)] = new_empty_row\n",
        "      else:\n",
        "        for i in range(0, hit_num):\n",
        "          abstract = resd['response']['docs'][i][\"abstract\"]\n",
        "          lead_paragraph = resd['response']['docs'][i][\"lead_paragraph\"]\n",
        "          pub_date = resd['response']['docs'][i][\"pub_date\"]\n",
        "          combined_text = abstract + \" \" + lead_paragraph\n",
        "          new_row = [state, quarters[quarter_iter], combined_text]\n",
        "          #print(new_row)\n",
        "          df.loc[len(df.index)] = new_row\n",
        "      print(state, quarters[quarter_iter], page)\n",
        "      time.sleep(12)\n",
        "\n",
        "df.to_csv('list3.csv', index=False)\n",
        "print(\"Done\")\n",
        "\n",
        "## DOWNLOAD THE CSVs AND UPLOAD TO GITHUB SO YOU DON'T HAVE TO RUN IT AGAIN"
      ],
      "metadata": {
        "id": "I0TDiNJliYid"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## API Search Code 4\n",
        "df = pd.DataFrame({\"State\": [], \"Quarter\": [], \"Text\": []})\n",
        "\n",
        "for state in states4:\n",
        "  for quarter_iter in range(0, len(quarter_dates), 2):\n",
        "    for page in range(1, 21):\n",
        "      pa={\"api-key\":apikey, \"q\": \"Health, \" + state, \"begin_date\": quarter_dates[quarter_iter], \"end_date\": quarter_dates[quarter_iter + 1], \"page\": page}\n",
        "      url2check = baseurl + urllib.parse.urlencode(pa)\n",
        "      result = urllib.request.urlopen(url2check).read()\n",
        "      resd = json.loads(result)\n",
        "      if len(resd['response']['docs']) < 10:\n",
        "        hit_num = len(resd['response']['docs'])\n",
        "      else:\n",
        "        hit_num = 10\n",
        "\n",
        "      if hit_num == 0:\n",
        "        new_empty_row = [state, quarters[quarter_iter], \"\"]\n",
        "        #print(new_empty_row)\n",
        "        df.loc[len(df.index)] = new_empty_row\n",
        "      else:\n",
        "        for i in range(0, hit_num):\n",
        "          abstract = resd['response']['docs'][i][\"abstract\"]\n",
        "          lead_paragraph = resd['response']['docs'][i][\"lead_paragraph\"]\n",
        "          pub_date = resd['response']['docs'][i][\"pub_date\"]\n",
        "          combined_text = abstract + \" \" + lead_paragraph\n",
        "          new_row = [state, quarters[quarter_iter], combined_text]\n",
        "          #print(new_row)\n",
        "          df.loc[len(df.index)] = new_row\n",
        "      print(state, quarters[quarter_iter], page)\n",
        "      time.sleep(12)\n",
        "\n",
        "df.to_csv('list4.csv', index=False)\n",
        "print(\"Done\")\n",
        "\n",
        "## DOWNLOAD THE CSVs AND UPLOAD TO GITHUB SO YOU DON'T HAVE TO RUN IT AGAIN"
      ],
      "metadata": {
        "id": "NiI4TFpsias_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## API Search Code 5\n",
        "df = pd.DataFrame({\"State\": [], \"Quarter\": [], \"Text\": []})\n",
        "\n",
        "for state in states5:\n",
        "  for quarter_iter in range(0, len(quarter_dates), 2):\n",
        "    for page in range(1, 21):\n",
        "      pa={\"api-key\":apikey, \"q\": \"Health, \" + state, \"begin_date\": quarter_dates[quarter_iter], \"end_date\": quarter_dates[quarter_iter + 1], \"page\": page}\n",
        "      url2check = baseurl + urllib.parse.urlencode(pa)\n",
        "      result = urllib.request.urlopen(url2check).read()\n",
        "      resd = json.loads(result)\n",
        "      if len(resd['response']['docs']) < 10:\n",
        "        hit_num = len(resd['response']['docs'])\n",
        "      else:\n",
        "        hit_num = 10\n",
        "\n",
        "      if hit_num == 0:\n",
        "        new_empty_row = [state, quarters[quarter_iter], \"\"]\n",
        "        #print(new_empty_row)\n",
        "        df.loc[len(df.index)] = new_empty_row\n",
        "      else:\n",
        "        for i in range(0, hit_num):\n",
        "          abstract = resd['response']['docs'][i][\"abstract\"]\n",
        "          lead_paragraph = resd['response']['docs'][i][\"lead_paragraph\"]\n",
        "          pub_date = resd['response']['docs'][i][\"pub_date\"]\n",
        "          combined_text = abstract + \" \" + lead_paragraph\n",
        "          new_row = [state, quarters[quarter_iter], combined_text]\n",
        "          #print(new_row)\n",
        "          df.loc[len(df.index)] = new_row\n",
        "      print(state, quarters[quarter_iter], page)\n",
        "      time.sleep(12)\n",
        "\n",
        "df.to_csv('list5.csv', index=False)\n",
        "print(\"Done\")\n",
        "\n",
        "## DOWNLOAD THE CSVs AND UPLOAD TO GITHUB SO YOU DON'T HAVE TO RUN IT AGAIN"
      ],
      "metadata": {
        "id": "fJKHcTSLidaD"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}